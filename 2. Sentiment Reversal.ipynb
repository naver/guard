{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a42c9ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "from disco.scorers import BooleanScorer\n",
    "from disco.distributions import LMDistribution\n",
    "from disco.distributions.context_distribution import ContextDistribution\n",
    "from disco.samplers import AccumulationSampler\n",
    "from disco.tuners import DPGTuner\n",
    "from disco.tuners.loggers.console import ConsoleLogger\n",
    "import torch\n",
    "from transformers import (\n",
    "    AutoModelForSequenceClassification,\n",
    "    AutoTokenizer,\n",
    "    TrainingArguments,\n",
    "    AutoModelForCausalLM,\n",
    ")\n",
    "import sympy as sp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "866eaf12",
   "metadata": {},
   "source": [
    "# Basic experimental setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "565f6705",
   "metadata": {},
   "source": [
    "define b(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b371dc5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "score_tokenizer = AutoTokenizer.from_pretrained(\"michellejieli/emotion_text_classifier\")\n",
    "score_model = AutoModelForSequenceClassification.from_pretrained(\"michellejieli/emotion_text_classifier\", num_labels=7).to('cuda')\n",
    "\n",
    "def sentiment_pipe(story): # joy_class is 4th class of output probabilities.\n",
    "    return Softmax(dim=-1)(score_model(**score_tokenizer(story, return_tensors=\"pt\", max_length=512, truncation=True).to('cuda')).logits)[:,3].item()\n",
    "\n",
    "\n",
    "def is_positive(story=\"\", t=0.98, prefix=\"\"):\n",
    "    story = prefix+story\n",
    "    story = story.split('<|endoftext|>')[0]\n",
    "    story = story.split('. ')\n",
    "    if sentiment_pipe(story[-1]) > t:\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "b = lambda s, c: is_positive(story=s.text, t=0.98, prefix=prefix) # hard constraint"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f38fa94",
   "metadata": {},
   "source": [
    "Negative openings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bfec9ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "prefix = ContextDistribution('datasets/ROC-negative.txt')\n",
    "prefix.contexts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12c481a9",
   "metadata": {},
   "source": [
    "# CAP experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39199bf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = LMDistribution(\"msintaha/gpt2-finetuned-rocstories\", LLM=True, length = 80)\n",
    "prefix = \"My uncle couldn't afford health care. He got sick last year.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "590f4a30",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = prefix\n",
    "distr = AccumulationSampler(distribution=a, total_size=500000) # In this part, we define the sampling size.\n",
    "samples_a, distr_a = distr.sample(sampling_size=500, context=prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cdca720",
   "metadata": {},
   "outputs": [],
   "source": [
    "CAP = \"This is my happy ending story, \"\n",
    "CAP += prefix\n",
    "CAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b267a213",
   "metadata": {},
   "outputs": [],
   "source": [
    "distr = AccumulationSampler(distribution=a, total_size=200000) # In this part, we define the sampling size.\n",
    "samples_a2, distr_a2 = distr.sample(sampling_size=500, context=CAP) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8303fb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "Z = len(samples_g) / len(samples_a)\n",
    "Z2 = len(samples_g2) / len(samples_a2)\n",
    "print('Z and log(Z):', Z, sp.log(Z))\n",
    "print(\"Z' and log(Z'):\", Z2, sp.log(Z2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ef135de",
   "metadata": {},
   "outputs": [],
   "source": [
    "score = []\n",
    "for it, item in enumerate(samples_g):\n",
    "    score.append(a.log_score([samples_g[it]], context=prompt) - a.log_score([samples_g[it]], context=gop))\n",
    "        \n",
    "estimator_g = sum(score)/len(score)\n",
    "\n",
    "print(estimator_g)\n",
    "\n",
    "score2 = []\n",
    "for it, item in enumerate(samples_g2):\n",
    "    score2.append(a.log_score([samples_g2[it]], context=gop) - a.log_score([samples_g2[it]], context=prompt))\n",
    "        \n",
    "estimator_g2 = sum(score2)/len(score2)\n",
    "\n",
    "print(estimator_g2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ab39461",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"KL(g|a')\", estimator_g-sp.log(Z))\n",
    "print(\"AR upgrade: \", Z2/Z)\n",
    "print(\"KL(g|g')\", estimator_g-sp.log(Z)+sp.log(Z2))\n",
    "print(\"KL(g'|g)\", estimator_g2-sp.log(Z2)+sp.log(Z))\n",
    "print(\"AR of a2: \", Z2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e28b1f7",
   "metadata": {},
   "source": [
    "# DPG Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bbd3a13",
   "metadata": {},
   "outputs": [],
   "source": [
    "proposal_model_name = \"msintaha/gpt2-finetuned-rocstories\"\n",
    "model_name = \"msintaha/gpt2-finetuned-rocstories\"\n",
    "# model_name = \"models/joy/gop-dpg-400k\"\n",
    "prefix = \"My uncle couldn't afford health care. He got sick last year.\"\n",
    "prefix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bed5f4f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = prefix\n",
    "prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0f8ed90",
   "metadata": {},
   "outputs": [],
   "source": [
    "token = \"\" # your own huggingface token\n",
    "\n",
    "proposal = LMDistribution(proposal_model_name, token=token, LLM=True, length=80)\n",
    "a2 = LMDistribution(model_name, token=token, freeze=False, LLM=True, length=80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f7c4bf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "b = lambda s, c: is_positive(story=s.text, t=0.98, prefix=prefix) # hard constraint\n",
    "scorer = BooleanScorer(b)\n",
    "g = proposal * scorer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "761f2361",
   "metadata": {},
   "outputs": [],
   "source": [
    "tuner = DPGTuner(a2, g,\n",
    "        warmup_steps=1,\n",
    "        context=prompt,\n",
    "        n_gradient_steps=200,\n",
    "        n_samples_per_step=10000,\n",
    "        sampling_size=500,\n",
    "        scoring_size=500,\n",
    "        divergence_evaluation_interval=10)\n",
    "\n",
    "ConsoleLogger(tuner)\n",
    "\n",
    "tuner.tune()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f67e170",
   "metadata": {},
   "outputs": [],
   "source": [
    "a2.save(path=\"models/positive/dpg-2m\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee9905d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name=\"msintaha/gpt2-finetuned-rocstories\"\n",
    "a = LMDistribution(model_name, token=token, LLM=True, length=80)\n",
    "\n",
    "distr = AccumulationSampler(distribution=a, total_size=500000)\n",
    "samples_a, distr_a = distr.sample(sampling_size=500, context=prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "265b9ecf",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name=\"models/positive/dpg-2m\"\n",
    "a2 = LMDistribution(model_name, token=token, LLM=True, length=80)\n",
    "\n",
    "distr = AccumulationSampler(distribution=a2, total_size=200000)\n",
    "samples_a2, distr_a2 = distr.sample(sampling_size=500, context=prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdc6f073",
   "metadata": {},
   "outputs": [],
   "source": [
    "samples_g = []\n",
    "distr_g = []\n",
    "for i in range(len(samples_a)):\n",
    "    if b(samples_a[i], _):\n",
    "        samples_g.append(samples_a[i])\n",
    "        distr_g.append(distr_a[i])\n",
    "len(samples_g)\n",
    "\n",
    "samples_g2 = []\n",
    "distr_g2 = []\n",
    "for i in range(len(samples_a2)):\n",
    "    if b(samples_a2[i], _):\n",
    "        samples_g2.append(samples_a2[i])\n",
    "        distr_g2.append(distr_a2[i])\n",
    "len(samples_g2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32408a96",
   "metadata": {},
   "outputs": [],
   "source": [
    "Z = len(samples_g) / len(samples_a)\n",
    "Z2 = len(samples_g2) / len(samples_a2)\n",
    "print('Z and log(Z):', Z, sp.log(Z))\n",
    "print(\"Z' and log(Z'):\", Z2, sp.log(Z2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f88a4715",
   "metadata": {},
   "outputs": [],
   "source": [
    "score = []\n",
    "for it, item in enumerate(samples_g):\n",
    "    score.append(a.log_score([samples_g[it]], context=prompt) - a2.log_score([samples_g[it]], context=prompt))\n",
    "        \n",
    "estimator_g = sum(score)/len(score)\n",
    "\n",
    "print(estimator_g)\n",
    "\n",
    "score2 = []\n",
    "for it, item in enumerate(samples_g2):\n",
    "    score2.append(a2.log_score([samples_g2[it]], context=prompt) - a.log_score([samples_g2[it]], context=prompt))\n",
    "        \n",
    "estimator_g2 = sum(score2)/len(score2)\n",
    "\n",
    "print(estimator_g2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d17da820",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"KL(g|a')\", estimator_g-sp.log(Z))\n",
    "print(\"AR upgrade: \", Z2/Z)\n",
    "print(\"KL(g|g')\", estimator_g-sp.log(Z)+sp.log(Z2))\n",
    "print(\"KL(g'|g)\", estimator_g2-sp.log(Z2)+sp.log(Z))\n",
    "print(\"AR of a2: \", Z2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a69c216",
   "metadata": {},
   "source": [
    "# SFT Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc1c34ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"msintaha/gpt2-finetuned-rocstories\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47a82dfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "prefix = \"My uncle couldn't afford health care. He got sick last year.\"\n",
    "prefix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7279d9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = prefix\n",
    "prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba36b6f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "token = \"\" # your own huggingface token\n",
    "\n",
    "a = LMDistribution(model_name, token=token, mb_mode=True, length=80)\n",
    "b = lambda s, c: is_positive(story=s.text, t=0.98, prefix=prefix) # hard constraint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f73a000f",
   "metadata": {},
   "outputs": [],
   "source": [
    "distr = AccumulationSampler(distribution=a, total_size=2000000) # 400,000\n",
    "samples_a, distr_a = distr.sample(sampling_size=500, context=prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8407e9b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "samples_g = []\n",
    "\n",
    "for it, item in enumerate(samples_a[:2000000]): # y ~ a\n",
    "    if b(item, _): # if b(y) = 1\n",
    "        samples_g.append({'text': prompt + item[1]}) # return y\n",
    "\n",
    "print(len(samples_g))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "161b81ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "from trl import SFTTrainer\n",
    "from datasets import load_dataset, Dataset\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='models/happy/sft',\n",
    "    per_device_train_batch_size=64,\n",
    "    num_train_epochs=1,\n",
    "    learning_rate=5e-06,\n",
    "    evaluation_strategy='steps',\n",
    "    logging_steps=100,\n",
    "    bf16=True,\n",
    "    report_to=\"none\",\n",
    ")\n",
    "\n",
    "ds_train = Dataset.from_list(samples_g) \n",
    "ds_val = Dataset.from_list(samples_g) \n",
    "ds_train = ds_train.shuffle()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12ef61e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
    "a0 = AutoModelForCausalLM.from_pretrained(model_name, device_map=\"auto\", trust_remote_code=True)\n",
    "# a.to('cuda')\n",
    "\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"right\"\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model=a0,\n",
    "    args=training_args,\n",
    "    train_dataset=ds_train,\n",
    "    eval_dataset=ds_val,\n",
    "    max_seq_length=1200,\n",
    "    tokenizer=tokenizer,\n",
    "    dataset_text_field='text',\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff5aa0f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()\n",
    "trainer.save_model(\"models/positive/sft-2m\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3a8ac24",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = LMDistribution(\"msintaha/gpt2-finetuned-rocstories\", token=token, LLM=True, length=80)\n",
    "\n",
    "distr = AccumulationSampler(distribution=a, total_size=500000)\n",
    "samples_a, distr_a = distr.sample(sampling_size=500, context=prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4e70593",
   "metadata": {},
   "outputs": [],
   "source": [
    "a2 = LMDistribution(\"models/positive/sft-2m\", token=token, LLM=True, length=80)\n",
    "\n",
    "distr = AccumulationSampler(distribution=a2, total_size=200000)\n",
    "samples_a2, distr_a2 = distr.sample(sampling_size=500, context=prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3dfc1f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "samples_g = []\n",
    "distr_g = []\n",
    "for i in range(len(samples_a)):\n",
    "    if b(samples_a[i], _):\n",
    "        samples_g.append(samples_a[i])\n",
    "        distr_g.append(distr_a[i])\n",
    "len(samples_g)\n",
    "\n",
    "samples_g2 = []\n",
    "distr_g2 = []\n",
    "for i in range(len(samples_a2)):\n",
    "    if b(samples_a2[i], _):\n",
    "        samples_g2.append(samples_a2[i])\n",
    "        distr_g2.append(distr_a2[i])\n",
    "len(samples_g2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfb46d85",
   "metadata": {},
   "outputs": [],
   "source": [
    "Z = len(samples_g) / len(samples_a)\n",
    "Z2 = len(samples_g2) / len(samples_a2)\n",
    "print('Z and log(Z):', Z, sp.log(Z))\n",
    "print(\"Z' and log(Z'):\", Z2, sp.log(Z2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c080640",
   "metadata": {},
   "outputs": [],
   "source": [
    "score = []\n",
    "for it, item in enumerate(samples_g):\n",
    "    score.append(a.log_score([samples_g[it]], context=prompt) - a2.log_score([samples_g[it]], context=prompt))\n",
    "        \n",
    "estimator_g = sum(score)/len(score)\n",
    "\n",
    "print(estimator_g)\n",
    "\n",
    "score2 = []\n",
    "for it, item in enumerate(samples_g2):\n",
    "    score2.append(a2.log_score([samples_g2[it]], context=prompt) - a.log_score([samples_g2[it]], context=prompt))\n",
    "        \n",
    "estimator_g2 = sum(score2)/len(score2)\n",
    "\n",
    "print(estimator_g2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5b4b62e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Delta\", -estimator_g)\n",
    "print(\"KL(g|a')\", estimator_g-sp.log(Z))\n",
    "print(\"AR of a2: \", Z2)\n",
    "print(\"KL(g|g')\", estimator_g-sp.log(Z)+sp.log(Z2))\n",
    "print(\"KL(g'|g)\", estimator_g2-sp.log(Z2)+sp.log(Z))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7044dfad",
   "metadata": {},
   "source": [
    "# warm-start DPG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0943b6d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"msintaha/gpt2-finetuned-rocstories\"\n",
    "prefix = \"My uncle couldn't afford health care. He got sick last year.\"\n",
    "\n",
    "token = \"\" # your own huggingface token\n",
    "\n",
    "a = LMDistribution(model_name, token=token, mb_mode=True, length=80)\n",
    "b = lambda s, c: is_positive(story=s.text, t=0.98, prefix=prefix) # hard constraint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ef43673",
   "metadata": {},
   "outputs": [],
   "source": [
    "CAP = \"This is my happy ending story, \"\n",
    "CAP += prefix\n",
    "CAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0a94f2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "distr = AccumulationSampler(distribution=a, total_size=10000) # 400,000\n",
    "samples_a, distr_a = distr.sample(sampling_size=500, context=CAP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2e296d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "samples_g = []\n",
    "\n",
    "for it, item in enumerate(samples_a[:10000]): # y ~ a\n",
    "    if b(item, _): # if b(y) = 1\n",
    "        samples_g.append({'text': prefix + item[1]}) # return y\n",
    "\n",
    "print(len(samples_g))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5493a33a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from trl import SFTTrainer\n",
    "from datasets import load_dataset, Dataset\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='models/happy/sft',\n",
    "    per_device_train_batch_size=24,\n",
    "    num_train_epochs=1,\n",
    "    learning_rate=5e-06,\n",
    "    evaluation_strategy='steps',\n",
    "    logging_steps=100,\n",
    "    bf16=True,\n",
    "    report_to=\"none\",\n",
    ")\n",
    "\n",
    "ds_train = Dataset.from_list(samples_g) \n",
    "ds_val = Dataset.from_list(samples_g) \n",
    "ds_train = ds_train.shuffle()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "167e1a6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
    "a0 = AutoModelForCausalLM.from_pretrained(model_name, device_map=\"auto\", trust_remote_code=True)\n",
    "# a.to('cuda')\n",
    "\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"right\"\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model=a0,\n",
    "    args=training_args,\n",
    "    train_dataset=ds_train,\n",
    "    eval_dataset=ds_val,\n",
    "    max_seq_length=70,\n",
    "    tokenizer=tokenizer,\n",
    "    dataset_text_field='text',\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "898506af",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()\n",
    "trainer.save_model(\"models/positive/ws\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1614763f",
   "metadata": {},
   "outputs": [],
   "source": [
    "proposal_model_name = \"msintaha/gpt2-finetuned-rocstories\"\n",
    "model_name = \"models/positive/ws\"\n",
    "prefix = \"My uncle couldn't afford health care. He got sick last year.\"\n",
    "prompt = prefix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18e8e70a",
   "metadata": {},
   "outputs": [],
   "source": [
    "token = \"\" # your own huggingface token\n",
    "\n",
    "proposal = LMDistribution(proposal_model_name, token=token, LLM=True, length=80)\n",
    "a2 = LMDistribution(model_name, token=token, freeze=False, LLM=True, length=80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a869bf6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "b = lambda s, c: is_positive(story=s.text, t=0.98, prefix=prefix) # hard constraint\n",
    "scorer = BooleanScorer(b)\n",
    "g = proposal * scorer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be9bbd21",
   "metadata": {},
   "outputs": [],
   "source": [
    "tuner = DPGTuner(a2, g,\n",
    "        warmup_steps=1,\n",
    "        context=prompt,\n",
    "        n_gradient_steps=199,\n",
    "        n_samples_per_step=10000,\n",
    "        sampling_size=500,\n",
    "        scoring_size=500,\n",
    "        divergence_evaluation_interval=10)\n",
    "\n",
    "ConsoleLogger(tuner)\n",
    "\n",
    "tuner.tune()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30569a59",
   "metadata": {},
   "outputs": [],
   "source": [
    "a2.save(path=\"models/positive/wsdpg-2m\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "577d8e7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name=\"msintaha/gpt2-finetuned-rocstories\"\n",
    "a = LMDistribution(model_name, token=token, LLM=True, length=80)\n",
    "\n",
    "distr = AccumulationSampler(distribution=a, total_size=500000)\n",
    "samples_a, distr_a = distr.sample(sampling_size=500, context=prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f58a474",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name=\"models/positive/wsdpg-2m\"\n",
    "a2 = LMDistribution(model_name, token=token, LLM=True, length=80)\n",
    "\n",
    "distr = AccumulationSampler(distribution=a2, total_size=200000)\n",
    "samples_a2, distr_a2 = distr.sample(sampling_size=500, context=prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10146d9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "samples_g = []\n",
    "distr_g = []\n",
    "for i in range(len(samples_a)):\n",
    "    if b(samples_a[i], _):\n",
    "        samples_g.append(samples_a[i])\n",
    "        distr_g.append(distr_a[i])\n",
    "len(samples_g)\n",
    "\n",
    "samples_g2 = []\n",
    "distr_g2 = []\n",
    "for i in range(len(samples_a2)):\n",
    "    if b(samples_a2[i], _):\n",
    "        samples_g2.append(samples_a2[i])\n",
    "        distr_g2.append(distr_a2[i])\n",
    "len(samples_g2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ecdc13d",
   "metadata": {},
   "outputs": [],
   "source": [
    "Z = len(samples_g) / len(samples_a)\n",
    "Z2 = len(samples_g2) / len(samples_a2)\n",
    "print('Z and log(Z):', Z, sp.log(Z))\n",
    "print(\"Z' and log(Z'):\", Z2, sp.log(Z2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e8b4e47",
   "metadata": {},
   "outputs": [],
   "source": [
    "score = []\n",
    "for it, item in enumerate(samples_g):\n",
    "    score.append(a.log_score([samples_g[it]], context=prompt) - a2.log_score([samples_g[it]], context=prompt))\n",
    "        \n",
    "estimator_g = sum(score)/len(score)\n",
    "\n",
    "print(estimator_g)\n",
    "\n",
    "score2 = []\n",
    "for it, item in enumerate(samples_g2):\n",
    "    score2.append(a2.log_score([samples_g2[it]], context=prompt) - a.log_score([samples_g2[it]], context=prompt))\n",
    "        \n",
    "estimator_g2 = sum(score2)/len(score2)\n",
    "\n",
    "print(estimator_g2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "514b238b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Delta\", -estimator_g)\n",
    "print(\"KL(g|a')\", estimator_g-sp.log(Z))\n",
    "print(\"AR of a2: \", Z2)\n",
    "print(\"KL(g|g')\", estimator_g-sp.log(Z)+sp.log(Z2))\n",
    "print(\"KL(g'|g)\", estimator_g2-sp.log(Z2)+sp.log(Z))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "guage",
   "language": "python",
   "name": "guage"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
