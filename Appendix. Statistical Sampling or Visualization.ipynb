{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cf87334",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from disco.scorers import BooleanScorer\n",
    "from disco.distributions import LMDistribution\n",
    "from disco.samplers import AccumulationSampler, QuasiRejectionSampler\n",
    "import transformers\n",
    "import numpy as np\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4376c8a0",
   "metadata": {},
   "source": [
    "# QRS sampler in Disco library"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75738bb1",
   "metadata": {},
   "source": [
    "We recommend beta = $Z$, but we did not illustrate theoretical background. It is future work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fbe9bc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "beta=0.5\n",
    "sampler = QuasiRejectionSampler(target_ebm, model, beta=beta)\n",
    "samples, log_scores = sampler.sample(sampling_size=2**7)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67934a2a",
   "metadata": {},
   "source": [
    "However, to compare with MCMC, we should implement in the other way."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5d88e9e",
   "metadata": {},
   "source": [
    "# Experimental Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b981b66e",
   "metadata": {},
   "source": [
    "For this experiment, if you want to analyze more clearly, we recommend the bad proposal models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "915ae801",
   "metadata": {},
   "outputs": [],
   "source": [
    "token = \"\" # your huggingface token\n",
    "a = LMDistribution(\"models/gemma-2b\", token=token, LLM=True)\n",
    "b = lambda s, c: bool(re.search(r\"\\bamazing\\b\", s.text)) # hard constraint\n",
    "a2 = LMDistribution(\"models/amazing/dpg-fail\", token=token, LLM=True) # Recipe: very low batch_size for DPG training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ccc4925",
   "metadata": {},
   "outputs": [],
   "source": [
    "scorer = BooleanScorer(b)\n",
    "g = a * scorer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db4dbec6",
   "metadata": {},
   "outputs": [],
   "source": [
    "distr = AccumulationSampler(distribution=a2, total_size=500000)\n",
    "samples_q2, distr_q2 = distr.sample(sampling_size=500, context=\"\")\n",
    "len(samples_q2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cf09dc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "start = []\n",
    "for i in range(len(samples_q2)):\n",
    "    if b(samples_q2[i],_) :\n",
    "        start.append(samples_q2[i])\n",
    "len(start)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8c7866d",
   "metadata": {},
   "source": [
    "With a lot of samples, let's track how distribution will change!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70b95f25",
   "metadata": {},
   "outputs": [],
   "source": [
    "def partition(text_list):\n",
    "    output = [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
    "    for _, text in enumerate(text_list):\n",
    "        j = text[1].index('amazing')/len(text[1])\n",
    "        output[int(j//0.1)] += 1\n",
    "    output = [float(i)/sum(output) for i in output]\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f7022dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "partition(start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5030308c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def QRS_filter(target, beta, y_a2, loga2):\n",
    "    out = []\n",
    "    out_prob = []\n",
    "    for i in range(len(y_a2)):\n",
    "        target_log_scores = target.log_score(samples=[y_a2[i]], context=\"\").to(\"cuda\")\n",
    "        rs = torch.exp(target_log_scores - loga2[i]) / beta\n",
    "        us = torch.rand(len(rs)).to(\"cuda\")\n",
    "        if us<rs:\n",
    "            out.append(y_a2[i])\n",
    "            out_prob.append(loga2[i])\n",
    "#             out_prob.append(min(target.log_score, beta*loga2[i]))\n",
    "    return out, out_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be9a8adf",
   "metadata": {},
   "outputs": [],
   "source": [
    "samples_g2 = []\n",
    "distr_g2 = []\n",
    "for i in range(len(samples_q2)):\n",
    "    if b(samples_q2[i], _):\n",
    "        samples_g2.append(samples_q2[i])\n",
    "        distr_g2.append(distr_q2[i])\n",
    "len(samples_g2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32903d58",
   "metadata": {},
   "outputs": [],
   "source": [
    "samples_g3, distr_g3 = QRS_filter(target=g, beta=0.05, y_a2=samples_g2[:20000], loga2=distr_g2)\n",
    "\n",
    "print('AR degradation is ', len(samples_g3)/len(samples_g2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6924a22",
   "metadata": {},
   "outputs": [],
   "source": [
    "partition(samples_g3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a24901f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def IMH(text, ebm, proposal, gop, n=50):\n",
    "    samples_q = text\n",
    "    distr_q = proposal.log_score(samples_q, context=gop)\n",
    "    distr_P = ebm.log_score(samples_q, context=\"\")\n",
    "    for i in range(n):\n",
    "        distr = AccumulationSampler(distribution=proposal, total_size=len(distr_P))\n",
    "        samples_q2, distr_q2 = distr.sample(sampling_size=250, context=gop)\n",
    "        distr_P2 = ebm.log_score(samples_q2, context=\"\")\n",
    "        for i in range(len(distr_P)):\n",
    "            rs = torch.exp(distr_P2[i]-distr_q2[i]+distr_q[i]-distr_P[i])\n",
    "            us = torch.rand(1).to(\"cuda\")\n",
    "            if us<rs:\n",
    "                samples_q[i] = samples_q2[i]\n",
    "                distr_q[i] = distr_q2[i]\n",
    "                distr_P[i] = distr_P2[i]\n",
    "        # print(torch.mean(distr_P) - sum(distr_q)/len(distr_q))\n",
    "    return samples_q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "151e2f48",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(partition(start))\n",
    "IMH_out = IMH(text = start, ebm=g, proposal=a2, gop=gop, n=50)\n",
    "\n",
    "print('AR degradation is ', len(IMH_out)/len(start))\n",
    "print(partition(IMH_out))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "354f32c7",
   "metadata": {},
   "source": [
    "# Visualization as UMAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c49613f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunks(l, n):\n",
    "    for i in range(0, len(l), n):\n",
    "        yield l[i:i + n]\n",
    "\n",
    "def fetch_vectors(string_list, batch_size=64):\n",
    "    DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    tokenizer = transformers.DistilBertTokenizer.from_pretrained(\"distilbert/distilbert-base-uncased\")\n",
    "    model = transformers.DistilBertModel.from_pretrained(\"distilbert/distilbert-base-uncased\")\n",
    "    model.to(DEVICE)\n",
    "\n",
    "    fin_features = []\n",
    "    total = len(string_list) // batch_size + 1\n",
    "    for data in tqdm(chunks(string_list, batch_size), total=total):\n",
    "        tokenized = []\n",
    "        for x in data:\n",
    "            x = \" \".join(x.strip().split()[:300])\n",
    "            tok = tokenizer.encode(x, add_special_tokens=True)\n",
    "            tokenized.append(tok[:512])\n",
    "\n",
    "        max_len = 512\n",
    "        padded = np.array([i + [0] * (max_len - len(i)) for i in tokenized])\n",
    "        attention_mask = np.where(padded != 0, 1, 0)\n",
    "        input_ids = torch.tensor(padded).to(DEVICE)\n",
    "        attention_mask = torch.tensor(attention_mask).to(DEVICE)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            last_hidden_states = model(input_ids, attention_mask=attention_mask)\n",
    "\n",
    "        features = last_hidden_states[0][:, 0, :].cpu().numpy()\n",
    "        fin_features.append(features)\n",
    "\n",
    "    fin_features = np.vstack(fin_features)\n",
    "    return fin_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fd33eb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "gemma = pd.read_json('texts/gemma-amazing.jsonl', lines=True)\n",
    "prompt = pd.read_json('texts/prompt-amazing.jsonl', lines=True)\n",
    "dpg = pd.read_json('texts/dpg-amazing.jsonl', lines=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a51a449",
   "metadata": {},
   "outputs": [],
   "source": [
    "out = gemma['text'].tolist()[:3500]+prompt['text'].tolist()[:3500]+dpg['text'].tolist()[:3500]\n",
    "\n",
    "out_vec = fetch_vectors(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bacb79b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# out = TSNE(n_components=2, perplexity=200).fit_transform(out_vec)\n",
    "\n",
    "reducer = umap.UMAP()\n",
    "out_vec2 = StandardScaler().fit_transform(out_vec)\n",
    "out = reducer.fit_transform(out_vec2)\n",
    "\n",
    "print('finish')\n",
    "\n",
    "out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e48fe373",
   "metadata": {},
   "outputs": [],
   "source": [
    "length = 200\n",
    "\n",
    "data1 = go.Scatter(\n",
    "    x=out[:length, 0],\n",
    "    y=out[:length, 1],\n",
    "    mode=\"markers\",\n",
    "    name=\"Gemma g\",\n",
    "    marker=dict(color='red')\n",
    ")\n",
    "\n",
    "\n",
    "data4 = go.Scatter(\n",
    "    x=out[7000:7000+length, 0],\n",
    "    y=out[7000:7000+length, 1],\n",
    "    mode=\"markers\",\n",
    "    name=\"DPG g'\",\n",
    "    marker=dict(color='lime')\n",
    ")\n",
    "\n",
    "\n",
    "data2 = go.Scatter(\n",
    "    x=out[3500:3500+length, 0],\n",
    "    y=out[3500:3500+length, 1],\n",
    "    mode=\"markers\",\n",
    "    name=\"prompted g'\",\n",
    "    marker=dict(color='blue')\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "fig = go.Figure()\n",
    "fig.add_trace(data1)\n",
    "fig.add_trace(data4)\n",
    "fig.add_trace(data2)\n",
    "\n",
    "fig.update_layout(\n",
    "    margin=dict(l=0, r=0, t=0, b=0),\n",
    "    legend=dict( \n",
    "    orientation=\"h\",\n",
    "    font=dict(size=30),\n",
    "))\n",
    "fig.update_layout(height=500)\n",
    "fig.write_image(\"fig/lexical_umap.pdf\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "guage",
   "language": "python",
   "name": "guage"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
